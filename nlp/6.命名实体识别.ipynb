{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67f2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#全局变量\n",
    "hub_token = open('/root/hub_token.txt').read().strip()\n",
    "repo_id = 'lansinuote/nlp.6.named_entity_recognition'\n",
    "push_to_hub = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2e6ab7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7592, 1010, 2023, 2003, 2034, 6251, 3975, 2046, 2616, 1012, 102], [101, 2023, 2003, 2117, 6251, 3975, 2046, 2616, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载编码器\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased',\n",
    "                                          use_fast=True)\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "#编码试算\n",
    "tokenizer.batch_encode_plus([[\n",
    "    'Hello', ',', 'this', 'is', 'first', 'sentence', 'split', 'into', 'words',\n",
    "    '.'\n",
    "], ['This', 'is', 'second', 'sentence', 'split', 'into', 'words', '.']],\n",
    "                            is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49df554b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "就算设置了is_split_into_words=True,编码后的数字和原文里的单词也不一定是一一对应的\n",
      "以下面这个数据为例\n",
      "{'id': '2', 'tokens': ['BRUSSELS', '1996-08-22'], 'pos_tags': [22, 11], 'chunk_tags': [11, 12], 'ner_tags': [5, 0]}\n",
      "调用编码\n",
      "可以看到原文里只有2个单词,但是编码后是7个词\n",
      "{'input_ids': [[101, 9371, 2727, 1011, 5511, 1011, 2570, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "恢复成文本之后长这样\n",
      "[CLS] brussels 1996 - 08 - 22 [SEP]\n",
      "因为有这样的情况存在,所以编码后的数字和label不能做到一一对应\n",
      "但是可以在编码结果上使用word_ids函数得到每个编码对应的原文索引\n",
      "特殊标识的索引是None,很显然,因为特殊符号不来自于原文的任何位置\n",
      "[None, 0, 1, 1, 1, 1, 1, None]\n",
      "有了word_ids,就可以找到编码后的数字对应的label,可以做到一一对应\n",
      "特殊符号的label置为-100,这和模型的预训练情况有关\n",
      "[-100, 5, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "#可以直接使用我处理好的数据集,这段代码可以跳过不看\n",
    "#以下为原理说明性代码\n",
    "def _():\n",
    "    print('就算设置了is_split_into_words=True,编码后的数字和原文里的单词也不一定是一一对应的')\n",
    "    print('以下面这个数据为例')\n",
    "    data = {\n",
    "        'id': '2',\n",
    "        'tokens': ['BRUSSELS', '1996-08-22'],\n",
    "        'pos_tags': [22, 11],\n",
    "        'chunk_tags': [11, 12],\n",
    "        'ner_tags': [5, 0]\n",
    "    }\n",
    "    print(data)\n",
    "\n",
    "    print('调用编码')\n",
    "    data_encode = tokenizer.batch_encode_plus([data['tokens']],\n",
    "                                              is_split_into_words=True)\n",
    "\n",
    "    print('可以看到原文里只有2个单词,但是编码后是7个词')\n",
    "    print(data_encode)\n",
    "\n",
    "    print('恢复成文本之后长这样')\n",
    "    print(tokenizer.decode(data_encode['input_ids'][0]))\n",
    "\n",
    "    print('因为有这样的情况存在,所以编码后的数字和label不能做到一一对应')\n",
    "    print('但是可以在编码结果上使用word_ids函数得到每个编码对应的原文索引')\n",
    "    print('特殊标识的索引是None,很显然,因为特殊符号不来自于原文的任何位置')\n",
    "    print(data_encode.word_ids(batch_index=0))\n",
    "\n",
    "    print('有了word_ids,就可以找到编码后的数字对应的label,可以做到一一对应')\n",
    "    print('特殊符号的label置为-100,这和模型的预训练情况有关')\n",
    "    label = [\n",
    "        -100 if i == None else data['ner_tags'][i]\n",
    "        for i in data_encode.word_ids(batch_index=0)\n",
    "    ]\n",
    "    print(label)\n",
    "\n",
    "\n",
    "_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4355f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d168feb80e470294c10d7f23f5edb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看数据样例\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "}) {'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c924099b6f1e42929e5ef3a43c0e39bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-5642b953c0a4ff28.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-652229bad444d43f.arrow\n",
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8bf8ddcef44bfead679ec92ef3b286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dd249327a4461bb51fa344f535c126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d76f538e36240e9a2d50088504f1e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split validation to the Hub.\n",
      "Resuming upload of the dataset shards.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1925e2aa6b1439f91b92d3f129511f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n",
      "Resuming upload of the dataset shards.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bef3694ba38453885a845d8debc1225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbf4847509d47878462b30b54566ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/586 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration lansinuote--nlp.6.named_entity_recognition-a0ecf5001aea9e74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--nlp.6.named_entity_recognition-a0ecf5001aea9e74/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb58dcb5a17e44139f22c8e987dfadb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7efa66255844f40837d682bd513e7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/194k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7478af4eb05748cd9b199fd2bf1ac06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/774k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babc06c97dd544da8b4afdddaa59e7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/184k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb98ca70dc7469d971da1e54fb15295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--nlp.6.named_entity_recognition-a0ecf5001aea9e74/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6601a64dc7064ded8d48f1e3cbd93aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "}) {'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    #加载数据\n",
    "    dataset = load_dataset(path='conll2003')\n",
    "\n",
    "    print('查看数据样例')\n",
    "    print(dataset, dataset['train'][0])\n",
    "\n",
    "    #根据以上说明性代码,写出这个数据处理函数\n",
    "    def tokenize_and_align_labels(data):\n",
    "        #分词\n",
    "        data_encode = tokenizer.batch_encode_plus(data['tokens'],\n",
    "                                                  truncation=True,\n",
    "                                                  is_split_into_words=True)\n",
    "\n",
    "        data_encode['labels'] = []\n",
    "        for i in range(len(data['tokens'])):\n",
    "            label = []\n",
    "            for word_id in data_encode.word_ids(batch_index=i):\n",
    "                if word_id is None:\n",
    "                    label.append(-100)\n",
    "                else:\n",
    "                    label.append(data['ner_tags'][i][word_id])\n",
    "\n",
    "            data_encode['labels'].append(label)\n",
    "\n",
    "        return data_encode\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=1,\n",
    "        remove_columns=['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if push_to_hub:\n",
    "    dataset = get_dataset()\n",
    "    dataset.push_to_hub(repo_id=repo_id, token=hub_token)\n",
    "\n",
    "#直接使用我处理好的数据集\n",
    "dataset = load_dataset(path=repo_id)\n",
    "\n",
    "print(dataset, dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26000c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 35]) tensor([[ 101, 5865, 1018, 3190, 1017,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 6278, 1022, 5395, 1020,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "attention_mask torch.Size([8, 35]) tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "labels torch.Size([8, 35]) tensor([[-100,    3,    0,    3,    0, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    3,    0,    3,    0, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1755"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset['train'],\n",
    "    batch_size=8,\n",
    "    collate_fn=DataCollatorForTokenClassification(tokenizer),\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "for i, data in enumerate(loader):\n",
    "    break\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape, v[:2])\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72a6db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6636.9801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(2.2111, grad_fn=<NllLossBackward0>), torch.Size([8, 35, 9]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, DistilBertModel, PreTrainedModel, PretrainedConfig\n",
    "\n",
    "#加载模型\n",
    "#model = AutoModelForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_name))\n",
    "\n",
    "#定义下游任务模型\n",
    "class Model(PreTrainedModel):\n",
    "    config_class = PretrainedConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.pretrained = DistilBertModel.from_pretrained(\n",
    "            'distilbert-base-uncased')\n",
    "\n",
    "        #9 = len(dataset['train'].features['ner_tags'].feature.names)\n",
    "        self.fc = torch.nn.Sequential(torch.nn.Dropout(0.1),\n",
    "                                      torch.nn.Linear(768, 9))\n",
    "\n",
    "        #加载预训练模型的参数\n",
    "        parameters = AutoModelForTokenClassification.from_pretrained(\n",
    "            'distilbert-base-uncased', num_labels=9)\n",
    "        self.fc[1].load_state_dict(parameters.classifier.state_dict())\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        logits = self.pretrained(input_ids=input_ids,\n",
    "                                 attention_mask=attention_mask)\n",
    "        logits = logits.last_hidden_state\n",
    "\n",
    "        logits = self.fc(logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits.flatten(end_dim=1), labels.flatten())\n",
    "\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "\n",
    "model = Model(PretrainedConfig())\n",
    "\n",
    "#统计参数量\n",
    "print(sum(i.numel() for i in model.parameters()) / 10000)\n",
    "\n",
    "out = model(**data)\n",
    "\n",
    "out['loss'], out['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c924a315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "0.060054857802800635\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    #数据加载器\n",
    "    loader_test = torch.utils.data.DataLoader(\n",
    "        dataset=dataset['test'],\n",
    "        batch_size=16,\n",
    "        collate_fn=DataCollatorForTokenClassification(tokenizer),\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    outs = []\n",
    "    for i, data in enumerate(loader_test):\n",
    "        #计算\n",
    "        with torch.no_grad():\n",
    "            out = model(**data)\n",
    "\n",
    "        out = out['logits'].argmax(dim=2)\n",
    "\n",
    "        for j in range(16):\n",
    "            #使用attention_mask筛选label,很显然,不需要pad的预测结果\n",
    "            #另外首尾两个特殊符号也不需要预测结果\n",
    "            select = data['attention_mask'][j] == 1\n",
    "            labels.append(data['labels'][j][select][1:-1])\n",
    "            outs.append(out[j][select][1:-1])\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "\n",
    "        if i == 50:\n",
    "            break\n",
    "\n",
    "    #计算正确率\n",
    "    labels = torch.cat(labels)\n",
    "    outs = torch.cat(outs)\n",
    "\n",
    "    print((labels == outs).sum().item() / len(labels))\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1375fcad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pt39/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.1965272426605225 0.12121212121212122 1.998860398860399e-05\n",
      "50 0.48114097118377686 0.8389830508474576 1.941880341880342e-05\n",
      "100 0.4562705457210541 0.8601398601398601 1.8849002849002852e-05\n",
      "150 0.2223113477230072 0.9166666666666666 1.827920227920228e-05\n",
      "200 0.0960574522614479 0.9793103448275862 1.770940170940171e-05\n",
      "250 0.12752580642700195 0.9477611940298507 1.713960113960114e-05\n",
      "300 0.14465288817882538 0.9716981132075472 1.6569800569800573e-05\n",
      "350 0.0917036160826683 0.9774436090225563 1.6000000000000003e-05\n",
      "400 0.03730904683470726 0.9937106918238994 1.5430199430199432e-05\n",
      "450 0.117700956761837 0.9852941176470589 1.4860398860398862e-05\n",
      "500 0.10514380782842636 0.9803921568627451 1.4290598290598293e-05\n",
      "550 0.1947314739227295 0.9603960396039604 1.3720797720797722e-05\n",
      "600 0.15881414711475372 0.9622641509433962 1.3150997150997152e-05\n",
      "650 0.184824138879776 0.9182389937106918 1.2581196581196581e-05\n",
      "700 0.09858043491840363 0.9647887323943662 1.2011396011396012e-05\n",
      "750 0.005401435773819685 1.0 1.1441595441595444e-05\n",
      "800 0.05666942149400711 0.9692307692307692 1.0871794871794871e-05\n",
      "850 0.026167653501033783 1.0 1.0301994301994303e-05\n",
      "900 0.1685640811920166 0.9568345323741008 9.732193732193734e-06\n",
      "950 0.026112496852874756 0.9925373134328358 9.162393162393163e-06\n",
      "1000 0.1219647154211998 0.9703703703703703 8.592592592592593e-06\n",
      "1050 0.17831368744373322 0.9548387096774194 8.022792022792024e-06\n",
      "1100 0.10610807687044144 0.9629629629629629 7.452991452991454e-06\n",
      "1150 0.06789354979991913 0.989010989010989 6.883190883190884e-06\n",
      "1200 0.02905152179300785 0.9945054945054945 6.313390313390314e-06\n",
      "1250 0.09071675688028336 0.9710144927536232 5.743589743589743e-06\n",
      "1300 0.09008719772100449 0.974025974025974 5.173789173789175e-06\n",
      "1350 0.07764382660388947 0.9723502304147466 4.603988603988604e-06\n",
      "1400 0.020746534690260887 1.0 4.034188034188034e-06\n",
      "1450 0.08469238132238388 0.9907407407407407 3.4643874643874647e-06\n",
      "1500 0.15835464000701904 0.9518072289156626 2.894586894586895e-06\n",
      "1550 0.017745135352015495 1.0 2.324786324786325e-06\n",
      "1600 0.005288714077323675 1.0 1.7549857549857553e-06\n",
      "1650 0.06470424681901932 0.9841269841269841 1.1851851851851854e-06\n",
      "1700 0.034537073224782944 0.9841269841269841 6.153846153846155e-07\n",
      "1750 0.01724872924387455 1.0 4.5584045584045586e-08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e0b6b3ffec4fafba0863da2327fc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf038a802e4456ba7675061f8f572b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/266M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "\n",
    "#训练\n",
    "def train():\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = get_scheduler(name='linear',\n",
    "                              num_warmup_steps=0,\n",
    "                              num_training_steps=len(loader),\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for i, data in enumerate(loader):\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].to(device)\n",
    "            \n",
    "        out = model(**data)\n",
    "        loss = out['loss']\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            labels = []\n",
    "            outs = []\n",
    "            out = out['logits'].argmax(dim=2)\n",
    "            for j in range(8):\n",
    "                #使用attention_mask筛选label,很显然,不需要pad的预测结果\n",
    "                #另外首尾两个特殊符号也不需要预测结果\n",
    "                select = data['attention_mask'][j] == 1\n",
    "                labels.append(data['labels'][j][select][1:-1])\n",
    "                outs.append(out[j][select][1:-1])\n",
    "\n",
    "            #计算正确率\n",
    "            labels = torch.cat(labels)\n",
    "            outs = torch.cat(outs)\n",
    "            accuracy = (labels == outs).sum().item() / len(labels)\n",
    "\n",
    "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "            print(i, loss.item(), accuracy, lr)\n",
    "\n",
    "    model.to('cpu')\n",
    "\n",
    "\n",
    "if push_to_hub:\n",
    "    train()\n",
    "    model.push_to_hub(repo_id=repo_id, use_auth_token=hub_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed397281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f06b2193784940a51e9294947ffb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/105 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405995346c784fd3a13f26204071a3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/266M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "0.973296013314169\n"
     ]
    }
   ],
   "source": [
    "#直接使用我训练好的模型\n",
    "model = Model.from_pretrained(repo_id)\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
